use std.io

struct Pos {
   path Str
   row int
   col int
}

fn [<<](f *File, pos Pos) *File {
   return f << pos.path << ':' << pos.row << ':' << pos.col << ": "
}

alias Token_Kind int
const (
   TOKEN_EOF
   TOKEN_SYMBOL

   TOKEN_COLON
   TOKEN_LPAREN
   TOKEN_RPAREN
   TOKEN_LBRACE
   TOKEN_RBRACE

   TOKEN_TYPE
   TOKEN_VAR
   TOKEN_RULE
   TOKEN_RUN
   TOKEN_TRACE
   COUNT_TOKENS
)

assert COUNT_TOKENS == 12
fn str_from_token_kind(kind Token_Kind) Str {
   match kind {
      TOKEN_EOF => return "end of file"
      TOKEN_SYMBOL => return "symbol"

      TOKEN_COLON => return "':'"
      TOKEN_LPAREN => return "'('"
      TOKEN_RPAREN => return "')'"
      TOKEN_LBRACE => return "'{'"
      TOKEN_RBRACE => return "'}'"
      TOKEN_TYPE => return "keyword 'type'"
      TOKEN_VAR => return "keyword 'var'"
      TOKEN_RULE => return "keyword 'rule'"
      TOKEN_RUN => return "keyword 'run'"
      TOKEN_TRACE => return "keyword 'trace'"
      else => assert false
   }
   return ""
}

struct Token {
   kind Token_Kind
   data int
   pos Pos
   str Str
}

fn token_new(kind Token_Kind) Token {
   let token Token
   token.kind = kind
   return token
}

fn [<<](token Token, str Str) Token {
   token.str = str
   return token
}

struct Lexer {
   pos Pos
   str Str
   peeked bool
   buffer Token
   prev_row int
}

let lexer Lexer

fn lexer_open(path *char) {
   lexer.pos.path = str_from_cstr(path)

   if !read_file(&lexer.str, path) {
      if lexer.peeked {
         &stderr << lexer.buffer.pos
      }

      &stderr << "error: could not read file '" << lexer.pos.path << "'\n"
      exit(1)
   }

   lexer.pos.row = 1
   lexer.pos.col = 1
   lexer.peeked = false
}

fn lexer_buffer(token Token) {
   lexer.peeked = true
   lexer.buffer = token
}

fn lexer_advance() {
   if *lexer.str.data == '\n' {
      lexer.pos.row += 1
      lexer.pos.col = 1
   } else {
      lexer.pos.col += 1
   }

   lexer.str.data += 1 as *char
   lexer.str.size -= 1
}

fn lexer_consume() char {
   lexer_advance()
   return lexer.str.data[-1]
}

fn lexer_match(ch char) bool {
   if lexer.str.size > 0 && *lexer.str.data == ch {
      lexer_advance()
      return true
   }
   return false
}

fn error_invalid(name Str) {
   &stderr << lexer.pos << "error: invalid " << name << " '" << lexer.str.data[-1] << "'\n"
   exit(1)
}

fn error_unterminated(name Str) {
   &stderr << lexer.pos << "error: unterminated " << name << "\n"
   exit(1)
}

fn lexer_char(name Str) char {
   if lexer.str.size == 0 {
      error_unterminated(name)
   }
   return lexer_consume()
}

assert COUNT_TOKENS == 12
fn lexer_next() Token {
   if lexer.peeked {
      lexer.peeked = false
      lexer.prev_row = lexer.buffer.pos.row
      return lexer.buffer
   }

   for lexer.str.size > 0 {
      if isspace(*lexer.str.data) {
         lexer_advance()
      } else if lexer_match('/') {
         if lexer_match('/') {
            for lexer.str.size > 0 && *lexer.str.data != '\n' {
               lexer_advance()
            }
         }
      } else {
         break
      }
   }

   let token Token
   token.pos = lexer.pos
   token.str = lexer.str

   if lexer.str.size == 0 {
      token.kind = TOKEN_EOF
   } else {
      token.kind = TOKEN_SYMBOL

      let in_escape = false
      match lexer_consume() {
         '\'' => {
            for true {
               let ch = lexer_char("escape")
               if ch == '\'' {
                  break
               }
            }
            in_escape = true
         }

         ':' => token.kind = TOKEN_COLON
         '(' => token.kind = TOKEN_LPAREN
         ')' => token.kind = TOKEN_RPAREN
         '{' => token.kind = TOKEN_LBRACE
         '}' => token.kind = TOKEN_RBRACE
      }

      if token.kind == TOKEN_SYMBOL && !in_escape {
         for lexer.str.size > 0 && !isspace(*lexer.str.data) && *lexer.str.data != '(' && *lexer.str.data != ')' && *lexer.str.data != ':' {
            lexer_advance()
         }

         token.str.size -= lexer.str.size

         if token.str == "type" {
            token.kind = TOKEN_TYPE
         } else if token.str == "var" {
            token.kind = TOKEN_VAR
         } else if token.str == "rule" {
            token.kind = TOKEN_RULE
         } else if token.str == "run" {
            token.kind = TOKEN_RUN
         } else if token.str == "trace" {
            token.kind = TOKEN_TRACE
         }
      } else {
         token.str.size -= lexer.str.size
      }
   }

   lexer.prev_row = token.pos.row
   return token
}

fn lexer_peek() Token {
   let prev_row = lexer.prev_row
   if !lexer.peeked {
      lexer_buffer(lexer_next())
      lexer.prev_row = prev_row
   }
   return lexer.buffer
}

fn lexer_read(kind Token_Kind) bool {
   lexer_peek()
   lexer.peeked = lexer.buffer.kind != kind
   return !lexer.peeked
}

fn lexer_expect(kind Token_Kind) Token {
   let token = lexer_next()
   if token.kind != kind {
      &stderr << token.pos << "error: expected " << str_from_token_kind(kind) << ", got " << str_from_token_kind(token.kind) << "\n"
      exit(1)
   }
   return token
}

fn lexer_either(a Token_Kind, b Token_Kind) Token {
   let token = lexer_next()
   if token.kind != a && token.kind != b {
      &stderr << token.pos << "error: expected " << str_from_token_kind(a) << " or " << str_from_token_kind(b) << ", got " << str_from_token_kind(token.kind) << "\n"
      exit(1)
   }
   return token
}

fn lexer_peek_row(token *Token) bool {
   *token = lexer_peek()
   return token.pos.row == lexer.prev_row
}

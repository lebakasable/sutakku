use std.io

struct Pos {
   path Str
   row int
   col int
}

fn [<<](f *File, pos Pos) *File {
   return f << pos.path << ':' << pos.row << ':' << pos.col << ": "
}

alias Token_Kind int
const (
   TOKEN_EOF
   TOKEN_SYMBOL

   TOKEN_COLON
   TOKEN_LBRACE
   TOKEN_RBRACE
   TOKEN_LARROW
   TOKEN_RARROW

   TOKEN_TYPE
   TOKEN_VAR
   TOKEN_RULE
   TOKEN_RUN
   COUNT_TOKENS
)

assert COUNT_TOKENS == 11
fn str_from_token_kind(kind Token_Kind) Str {
   match kind {
      TOKEN_EOF => return "end of file"
      TOKEN_SYMBOL => return "symbol"

      TOKEN_COLON => return "':'"
      TOKEN_LBRACE => return "'{'"
      TOKEN_RBRACE => return "'}'"
      TOKEN_LARROW => return "'<-'"
      TOKEN_RARROW => return "'->'"
      TOKEN_TYPE => return "keyword 'type'"
      TOKEN_VAR => return "keyword 'var'"
      TOKEN_RULE => return "keyword 'rule'"
      TOKEN_RUN => return "keyword 'run'"
      else => assert false
   }
   return ""
}

struct Token {
   kind Token_Kind
   data int
   pos Pos
   str Str
}

fn token_new(kind Token_Kind) Token {
   let token Token
   token.kind = kind
   return token
}

fn [<<](token Token, str Str) Token {
   token.str = str
   return token
}

struct Lexer {
   pos Pos
   str Str
   peeked bool
   buffer Token
   prev_row int
}

let lexer Lexer

fn lexer_open(path *char) {
   lexer.pos.path = str_from_cstr(path)

   if !read_file(&lexer.str, path) {
      if lexer.peeked {
         &stderr << lexer.buffer.pos
      }

      &stderr << "error: could not read file '" << lexer.pos.path << "'\n"
      exit(1)
   }

   lexer.pos.row = 1
   lexer.pos.col = 1
   lexer.peeked = false
}

fn lexer_buffer(token Token) {
   lexer.peeked = true
   lexer.buffer = token
}

fn lexer_advance() {
   if *lexer.str.data == '\n' {
      lexer.pos.row += 1
      lexer.pos.col = 1
   } else {
      lexer.pos.col += 1
   }

   lexer.str.data += 1 as *char
   lexer.str.size -= 1
}

fn lexer_consume() char {
   lexer_advance()
   return lexer.str.data[-1]
}

fn lexer_match(ch char) bool {
   if lexer.str.size > 0 && *lexer.str.data == ch {
      lexer_advance()
      return true
   }
   return false
}

fn error_invalid(name Str) {
   &stderr << lexer.pos << "error: invalid " << name << " '" << lexer.str.data[-1] << "'\n"
   exit(1)
}

assert COUNT_TOKENS == 11
fn lexer_next() Token {
   if lexer.peeked {
      lexer.peeked = false
      lexer.prev_row = lexer.buffer.pos.row
      return lexer.buffer
   }

   for lexer.str.size > 0 {
      if isspace(*lexer.str.data) {
         lexer_advance()
      } else if lexer_match('#') {
         if lexer_match('#') {
            for lexer.str.size > 0 {
               if lexer_match('#') && lexer_match('#') {
                  break
               }

               lexer_advance()
            }
         } else {
            for lexer.str.size > 0 && *lexer.str.data != '\n' {
               lexer_advance()
            }
         }
      } else {
         break
      }
   }

   let token Token
   token.pos = lexer.pos
   token.str = lexer.str

   if lexer.str.size == 0 {
      token.kind = TOKEN_EOF
   } else {
      token.kind = TOKEN_SYMBOL

      match lexer_consume() {
         ':' => token.kind = TOKEN_COLON
         '{' => token.kind = TOKEN_LBRACE
         '}' => token.kind = TOKEN_RBRACE
         '<' => {
            if lexer_match('-') {
               token.kind = TOKEN_LARROW
            }
         }
         '-' => {
            if lexer_match('>') {
               token.kind = TOKEN_RARROW
            }
         }
      }

      if token.kind == TOKEN_SYMBOL {
         for lexer.str.size > 0 && !isspace(*lexer.str.data) {
            lexer_advance()
         }

         token.str.size -= lexer.str.size

         if token.str == "type" {
            token.kind = TOKEN_TYPE
         } else if token.str == "var" {
            token.kind = TOKEN_VAR
         } else if token.str == "rule" {
            token.kind = TOKEN_RULE
         } else if token.str == "run" {
            token.kind = TOKEN_RUN
         }
      } else {
         token.str.size -= lexer.str.size
      }
   }

   lexer.prev_row = token.pos.row
   return token
}

fn lexer_peek() Token {
   let prev_row = lexer.prev_row
   if !lexer.peeked {
      lexer_buffer(lexer_next())
      lexer.prev_row = prev_row
   }
   return lexer.buffer
}

fn lexer_read(kind Token_Kind) bool {
   lexer_peek()
   lexer.peeked = lexer.buffer.kind != kind
   return !lexer.peeked
}

fn lexer_expect(kind Token_Kind) Token {
   let token = lexer_next()
   if token.kind != kind {
      &stderr << token.pos << "error: expected " << str_from_token_kind(kind) << ", got " << str_from_token_kind(token.kind) << "\n"
      exit(1)
   }
   return token
}

fn lexer_either(a Token_Kind, b Token_Kind) Token {
   let token = lexer_next()
   if token.kind != a && token.kind != b {
      &stderr << token.pos << "error: expected " << str_from_token_kind(a) << " or " << str_from_token_kind(b) << ", got " << str_from_token_kind(token.kind) << "\n"
      exit(1)
   }
   return token
}

fn lexer_peek_row(token *Token) bool {
   *token = lexer_peek()
   return token.pos.row == lexer.prev_row
}
